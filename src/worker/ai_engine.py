import google.generativeai as genai
import PIL.Image
import json
import logging
from datetime import datetime
import pytz # C·∫ßn pip install pytz
from src.common.config import settings
from src.common.logger import logger
from src.common.template_registry import get_template_config

class AIEngine:
    def __init__(self):
        if not settings.GEMINI_API_KEY:
            logger.warning("GEMINI_API_KEY is not set. AI analysis will be disabled.")
            self.model = None
            return

        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
            logger.info(f"AI Engine initialized with model: {settings.GEMINI_MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize AI Engine: {e}")
            self.model = None

    async def generate_structured_report(self, messages: list, template_code: str) -> dict:
        """
        T·∫°o b√°o c√°o c√≥ c·∫•u tr√∫c (JSON) cho Visualizer.
        T√≠ch h·ª£p: Time Context + Strict JSON enforcement.
        """
        if not self.model:
            return None
            
        config = get_template_config(template_code)
        
        # 1. L·∫•y th·ªùi gian th·ª±c (Vi·ªát Nam)
        vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')
        now_str = datetime.now(vn_tz).strftime("%Y-%m-%d %H:%M:%S")
        
        # 2. Context t·ª´ tin nh·∫Øn
        # Gi·ªõi h·∫°n context window ƒë·ªÉ tr√°nh qu√° t·∫£i token, l·∫•y 50 tin m·ªõi nh·∫•t
        context_text = "\n---\n".join(messages[:50])
        
        # 3. Dynamic Prompt (ƒê√£ n√¢ng c·∫•p v·ªõi Time Context & Anti-Hallucination)
        system_prompt = f"""
        {config['ai_prompt']}
        
        --- B·ªêI C·∫¢NH TH·ªúI GIAN TH·ª∞C (QUAN TR·ªåNG) ---
        üïí TH·ªúI GIAN HI·ªÜN T·∫†I (VN): {now_str}
        
        LU·∫¨T B·∫ÆT BU·ªòC:
        1. KI·ªÇM TRA TH·ªúI GIAN: So s√°nh th·ªùi gian trong tin nh·∫Øn v·ªõi "TH·ªúI GIAN HI·ªÜN T·∫†I". 
           - N·∫øu tin nh·∫Øn n√≥i "h√¥m qua" ho·∫∑c qu√° 24h, h√£y coi l√† tin c≈© (tr·ª´ khi l√† ph√¢n t√≠ch vƒ© m√¥).
           - ∆Øu ti√™n tin nh·∫Øn m·ªõi nh·∫•t.
        2. CH·ªêNG B·ªäA ƒê·∫∂T (HALLUCINATION): 
           - Ch·ªâ tr√≠ch xu·∫•t con s·ªë (Gi√°, Volume) c√≥ trong vƒÉn b·∫£n. KH√îNG ƒê∆Ø·ª¢C ƒêO√ÅN.
           - N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, tr·∫£ v·ªÅ "N/A" ho·∫∑c null.
        3. OUTPUT JSON THU·∫¶N: 
           - Tr·∫£ v·ªÅ JSON h·ª£p l·ªá. Kh√¥ng d√πng Markdown (```json). Kh√¥ng gi·∫£i th√≠ch th√™m.
           - Field "metrics" ph·∫£i lu√¥n t·ªìn t·∫°i (d√π r·ªóng).
        
        D·ªØ li·ªáu ƒë·∫ßu v√†o (Tin nh·∫Øn Telegram):
        {context_text}
        """

        try:
            # TƒÉng temperature l√™n m·ªôt ch√∫t (0.4) ƒë·ªÉ AI linh ho·∫°t h∆°n trong vi·ªác t√≥m t·∫Øt, 
            # nh∆∞ng v·∫´n ƒë·ªß th·∫•p ƒë·ªÉ gi·ªØ c·∫•u tr√∫c JSON.
            generation_config = genai.types.GenerationConfig(
                temperature=0.4,
                response_mime_type="application/json" # √âp Gemini tr·∫£ v·ªÅ JSON Mode (t√≠nh nƒÉng m·ªõi)
            )

            response = await self.model.generate_content_async(
                system_prompt, 
                generation_config=generation_config
            )
            
            raw_text = response.text.strip()
            
            # --- Robust JSON Parsing ---
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p AI v·∫´n c·ªë t√¨nh tr·∫£ v·ªÅ Markdown code block
            if raw_text.startswith("```json"):
                raw_text = raw_text[7:]
            if raw_text.startswith("```"):
                raw_text = raw_text[3:]
            if raw_text.endswith("```"):
                raw_text = raw_text[:-3]
                
            data = json.loads(raw_text.strip())
            
            # Handle case where AI returns a list instead of a dict
            if isinstance(data, list):
                if len(data) > 0 and isinstance(data[0], dict):
                    data = data[0]
                else:
                    data = {"summary": str(data), "metrics": {}}

            # ƒê·∫£m b·∫£o c·∫•u tr√∫c d·ªØ li·ªáu cho Visualizer
            if "metrics" not in data: 
                data["metrics"] = {}
            
            # T·ª± ƒë·ªông t√≠nh to√°n/ƒëi·ªÅn c√°c field b·ªã thi·∫øu ƒë·ªÉ Visualizer kh√¥ng l·ªói
            if "score" not in data:
                data["score"] = 50 # Default neutral
            if "summary" not in data:
                data["summary"] = "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≥m t·∫Øt."

            return data
            
        except json.JSONDecodeError as e:
            logger.error(f"AI JSON Decode Error for {template_code}: {e} | Raw: {raw_text[:100]}...")
            return None
        except Exception as e:
            logger.error(f"AI Error for {template_code}: {e}")
            return None

    async def generate_text(self, prompt: str) -> str:
        """
        Generic method to generate text (d√πng cho c√°c task ph·ª•).
        """
        if not self.model: return ""
        try:
            response = await self.model.generate_content_async(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"AI Generation failed: {e}")
            return ""

    async def analyze_message(self, message_text: str, plan_type: str = "VIP") -> str:
        """
        Ph√¢n t√≠ch nhanh 1 tin nh·∫Øn l·∫ª (D√πng cho Bot chat tr·ª±c ti·∫øp).
        """
        if not self.model: return "AI Analysis Unavailable"

        if plan_type == "BUSINESS":
            prompt = f"""
            B·∫°n l√† chuy√™n gia Crypto Alpha Hunter. Ph√¢n t√≠ch tin nh·∫Øn sau:
            "{message_text}"
            
            Output Format Telegram:
            üìå **T√≥m t·∫Øt**: ...
            üìä **Ph√¢n t√≠ch**: ...
            üéØ **V√πng gi√°**: Entry/TP (n·∫øu c√≥)
            ‚≠ê **ƒêi·ªÉm**: 1-10
            üí° **Chi·∫øn l∆∞·ª£c**: ...
            """
        else:
            prompt = f"""
            Ph√¢n t√≠ch tin nh·∫Øn Crypto sau ng·∫Øn g·ªçn:
            "{message_text}"
            
            Output:
            - T√≥m t·∫Øt: ...
            - ƒê√°nh gi√°: 1-10
            - H√†nh ƒë·ªông: Mua/B√°n/Quan s√°t
            """

        try:
            response = await self.model.generate_content_async(prompt)
            return response.text.strip()
        except Exception as e:
            logger.error(f"AI Analysis failed: {e}")
            return "AI Analysis Failed"

    async def extract_text_from_image(self, image_path: str) -> str:
        """
        OCR ·∫£nh chart/k√®o (D√πng cho Ingestor).
        """
        if not self.model: return ""
        try:
            img = PIL.Image.open(image_path)
            prompt = "Extract details: Token, Entry, TP, SL, Direction (Long/Short). Return just text."
            response = await self.model.generate_content_async([prompt, img])
            return response.text.strip()
        except Exception as e:
            logger.error(f"AI OCR failed: {e}")
            return ""

ai_engine = AIEngine()
